{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "974271a6",
   "metadata": {},
   "source": [
    "# CRI CIFAR Demonstration with snnTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c830d120",
   "metadata": {},
   "source": [
    "## Training SNN with snnTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6414a490",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snntorch\n",
      "  Using cached snntorch-0.5.3-py2.py3-none-any.whl (95 kB)\n",
      "Collecting torch>=1.1.0\n",
      "  Using cached torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-1.24.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 112.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.2 MB 101.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: setuptools in ./l2s/.venv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.1.0->snntorch) (56.0.0)\n",
      "Requirement already satisfied: wheel in ./l2s/.venv/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.1.0->snntorch) (0.36.2)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 113.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.2.1\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting python-dateutil>=2.7\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "\u001b[K     |████████████████████████████████| 965 kB 131.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "\u001b[K     |████████████████████████████████| 299 kB 109.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.4.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 110.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.5\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "\u001b[K     |████████████████████████████████| 499 kB 107.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: six, nvidia-cublas-cu11, numpy, typing-extensions, pytz, python-dateutil, pyparsing, pillow, packaging, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, kiwisolver, fonttools, cycler, contourpy, torch, pandas, matplotlib, snntorch\n",
      "Successfully installed contourpy-1.0.7 cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.6.3 numpy-1.24.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 packaging-23.0 pandas-1.5.3 pillow-9.4.0 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2022.7.1 six-1.16.0 snntorch-0.5.3 torch-1.13.1 typing-extensions-4.4.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/Volumes/export/isn/keli/Desktop/CRI/l2s/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install snntorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad47b541",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2552234/4177840763.py:4: DeprecationWarning: The module snntorch.backprop will be deprecated in  a future release. Writing out your own training loop will lead to substantially faster performance.\n",
      "  from snntorch import backprop\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from quant_layer import weight_quantize_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8a486",
   "metadata": {},
   "source": [
    "### Import CIFAR datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f283ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataloader arguments\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2148c48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170498071/170498071 [00:05<00:00, 31976067.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor()])\n",
    "\n",
    "batch_size = 4\n",
    "subset = 10\n",
    "\n",
    "CIFAR_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "CIFAR_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(CIFAR_train,batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "test_loader = DataLoader(CIFAR_test,batch_size=batch_size,shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c078b195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = CIFAR_train.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8928b101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b1c2c",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2322fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = 28*28\n",
    "num_hidden_0 = 2500\n",
    "num_hidden_1 = 2000\n",
    "num_hidden_2 = 1500\n",
    "num_hidden_3 = 1000\n",
    "num_hidden_4 = 500\n",
    "num_hidden = 1000\n",
    "num_outputs = 10\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25\n",
    "beta = 1.0\n",
    "spike_grad = surrogate.sigmoid(slope=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1005cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(num_inputs, num_hidden, bias=True), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden, num_outputs, bias=True),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True, output=True)).to(device)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2956af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net6 = nn.Sequential(nn.Linear(num_inputs, num_hidden_0), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_0, num_hidden_1), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_1, num_hidden_2), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_2, num_hidden_3), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_3, num_hidden_4), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_4, num_outputs),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True, output=True)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "185a5d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_0 = 6000\n",
    "num_hidden_1 = 5500\n",
    "num_hidden_2 = 5000\n",
    "num_hidden_3 = 4500\n",
    "num_hidden_4 = 4000\n",
    "num_hidden_5 = 3500\n",
    "num_hidden_6 = 3000\n",
    "num_hidden_7 = 2500\n",
    "num_hidden_8 = 2000\n",
    "num_hidden_9 = 1500\n",
    "num_hidden_10 = 500\n",
    "num_hidden_11 = 100\n",
    "num_hidden_12 = 600\n",
    "num_hidden_13 = 400\n",
    "num_hidden_14 = 200\n",
    "num_hidden_15 = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d7318c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net12 = nn.Sequential(nn.Linear(num_inputs, num_hidden_0), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_0, num_hidden_1), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_1, num_hidden_2), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_2, num_hidden_3), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_3, num_hidden_4), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_4, num_hidden_5), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_5, num_hidden_6), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_6, num_hidden_7), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_7, num_hidden_8), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_8, num_hidden_9), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_9, num_hidden_10), \n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "#                     nn.Linear(num_hidden_10, num_hidden_11), \n",
    "#                     snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "#                     nn.Linear(num_hidden_11, num_hidden_12), \n",
    "#                     snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "#                     nn.Linear(num_hidden_12, num_hidden_13), \n",
    "#                     snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "#                     nn.Linear(num_hidden_13, num_hidden_14), \n",
    "#                     snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "#                     nn.Linear(num_hidden_14, num_hidden_15), \n",
    "#                     snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True),\n",
    "                    nn.Linear(num_hidden_10, num_outputs),\n",
    "                    snn.Leaky(beta=beta, spike_grad=spike_grad,init_hidden=True, output=True)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15f49a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3072, out_features=6000, bias=True)\n",
       "  (1): Leaky()\n",
       "  (2): Linear(in_features=6000, out_features=5500, bias=True)\n",
       "  (3): Leaky()\n",
       "  (4): Linear(in_features=5500, out_features=5000, bias=True)\n",
       "  (5): Leaky()\n",
       "  (6): Linear(in_features=5000, out_features=4500, bias=True)\n",
       "  (7): Leaky()\n",
       "  (8): Linear(in_features=4500, out_features=4000, bias=True)\n",
       "  (9): Leaky()\n",
       "  (10): Linear(in_features=4000, out_features=3500, bias=True)\n",
       "  (11): Leaky()\n",
       "  (12): Linear(in_features=3500, out_features=3000, bias=True)\n",
       "  (13): Leaky()\n",
       "  (14): Linear(in_features=3000, out_features=2500, bias=True)\n",
       "  (15): Leaky()\n",
       "  (16): Linear(in_features=2500, out_features=2000, bias=True)\n",
       "  (17): Leaky()\n",
       "  (18): Linear(in_features=2000, out_features=1500, bias=True)\n",
       "  (19): Leaky()\n",
       "  (20): Linear(in_features=1500, out_features=500, bias=True)\n",
       "  (21): Leaky()\n",
       "  (22): Linear(in_features=500, out_features=10, bias=True)\n",
       "  (23): Leaky()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5e41f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "data = data.to(device)\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1081d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, num_steps, data, batch_size):\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "    utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        spk_out, mem_out = net(data.view(batch_size, -1))\n",
    "        spk_rec.append(spk_out)\n",
    "        mem_rec.append(mem_out)\n",
    "  \n",
    "    return torch.stack(spk_rec), torch.stack(mem_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "404a3290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spk_rec, mem_rec = forward_pass(net6, num_steps, data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a4544bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 4, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spk_rec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07deb717",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "358fb267",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = SF.ce_rate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a8e3328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss from an untrained network is 2.299\n"
     ]
    }
   ],
   "source": [
    "loss_val = loss_fn(spk_rec, targets)\n",
    "\n",
    "print(f\"The loss from an untrained network is {loss_val.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899eecf",
   "metadata": {},
   "source": [
    "### Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98c576e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a single batch using an untrained network is 25.000%\n"
     ]
    }
   ],
   "source": [
    "acc = SF.accuracy_rate(spk_rec, targets)\n",
    "\n",
    "print(f\"The accuracy of a single batch using an untrained network is {acc*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d7e3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(train_loader, net, num_steps, batch_size):\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        acc = 0\n",
    "        net.eval()\n",
    "\n",
    "        train_loader = iter(train_loader)\n",
    "        for data, targets in train_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            spk_rec, _ = forward_pass(net, num_steps, data, batch_size)\n",
    "\n",
    "            acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "            total += spk_rec.size(1)\n",
    "\n",
    "    return acc/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab38e51",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ede3728",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net6.parameters(), lr=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a39c571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 50\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 100\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 150\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 200\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 250\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 25.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 300\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 350\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 400\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 25.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 450\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 500\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 25.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 550\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 600\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 25.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 650\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 25.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 700\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 750\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 25.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 800\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 50.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 850\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 25.00%\n",
      "Test set accuracy for a single minibatch: 25.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 900\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 950\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 1000\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 75.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 1050\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 1100\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 50.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 1150\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 0.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 1200\n",
      "Train Set Loss: 2.303\n",
      "Test Set Loss: 2.303\n",
      "Train set accuracy for a single minibatch: 25.00%\n",
      "Test set accuracy for a single minibatch: 0.00%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f32f2682dc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Volumes/export/isn/keli/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1430, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Volumes/export/isn/keli/miniconda3/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Volumes/export/isn/keli/miniconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/Volumes/export/isn/keli/miniconda3/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Volumes/export/isn/keli/miniconda3/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 322224, 322236) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/reductions.py:305\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 305\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     85\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m c\u001b[38;5;241m.\u001b[39msend((key, os\u001b[38;5;241m.\u001b[39mgetpid()))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/connection.py:507\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 507\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43mSocketClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(authkey, \u001b[38;5;28mbytes\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/connection.py:635\u001b[0m, in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    634\u001b[0m s\u001b[38;5;241m.\u001b[39msetblocking(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 635\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Connection(s\u001b[38;5;241m.\u001b[39mdetach())\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m train_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_loader)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Minibatch training loop\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, targets \u001b[38;5;129;01min\u001b[39;00m train_batch:\n\u001b[1;32m     13\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1132\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 322224, 322236) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net6.train()\n",
    "        spk_rec, mem_rec = forward_pass(net6, num_steps, data, batch_size)\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        loss_val += loss_fn(spk_rec, targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net6.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = forward_pass(net6, num_steps, test_data,batch_size)\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            test_loss += loss_fn(test_spk, test_targets)\n",
    "\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "                    print(f\"Train Set Loss: {loss_hist[counter]:.3f}\")\n",
    "                    print(f\"Test Set Loss: {test_loss_hist[counter]:.3f}\")\n",
    "                    train_acc = SF.accuracy_rate(spk_rec, targets)\n",
    "                    test_acc = SF.accuracy_rate(test_spk, test_targets)\n",
    "                    print(f\"Train set accuracy for a single minibatch: {train_acc*100:.2f}%\")\n",
    "                    print(f\"Test set accuracy for a single minibatch: {test_acc*100:.2f}%\")\n",
    "                    print(\"\\n\")\n",
    "            counter += 1\n",
    "            iter_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb8d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = batch_accuracy(test_loader, net6, num_steps, batch_size)\n",
    "\n",
    "print(f\"The total accuracy on the test set is: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6acb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "fig = plt.figure(facecolor=\"w\")\n",
    "plt.plot(test_acc_hist)\n",
    "plt.title(\"Test Set Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f80236a",
   "metadata": {},
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22377313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_quan, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_quan:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_quan.pth.tar'))\n",
    "    else:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2179c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa258d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint({'state_dict': net16.state_dict(),}, 0, fdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9605f20e",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_quantization(b):\n",
    "\n",
    "    def uniform_quant(x, b):\n",
    "        xdiv = x.mul((2 ** b - 1))\n",
    "        xhard = xdiv.round().div(2 ** b - 1)\n",
    "        #print('uniform quant bit: ', b)\n",
    "        return xhard\n",
    "\n",
    "    class _pq(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, alpha):\n",
    "            input.div_(alpha)                          # weights are first divided by alpha\n",
    "            input_c = input.clamp(min=-1, max=1)       # then clipped to [-1,1]\n",
    "            sign = input_c.sign()\n",
    "            input_abs = input_c.abs()\n",
    "            input_q = uniform_quant(input_abs, b).mul(sign)\n",
    "            ctx.save_for_backward(input, input_q)\n",
    "            input_q = input_q.mul(alpha)               # rescale to the original range\n",
    "            return input_q\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            grad_input = grad_output.clone()             # grad for weights will not be clipped\n",
    "            input, input_q = ctx.saved_tensors\n",
    "            i = (input.abs()>1.).float()     # >1 means clipped. # output matrix is a form of [True, False, True, ...]\n",
    "            sign = input.sign()              # output matrix is a form of [+1, -1, -1, +1, ...]\n",
    "            #grad_alpha = (grad_output*(sign*i + (input_q-input)*(1-i))).sum()\n",
    "            grad_alpha = (grad_output*(sign*i + (0.0)*(1-i))).sum()\n",
    "            # above line, if i = True,  and sign = +1, \"grad_alpha = grad_output * 1\"\n",
    "            #             if i = False, \"grad_alpha = grad_output * (input_q-input)\"\n",
    "            grad_input = grad_input*(1-i)\n",
    "            return grad_input, grad_alpha\n",
    "\n",
    "    return _pq().apply\n",
    "\n",
    "class weight_quantize_fn(nn.Module):\n",
    "    def __init__(self, w_bit):\n",
    "        super(weight_quantize_fn, self).__init__()\n",
    "        self.w_bit = w_bit-1\n",
    "        #self.wgt_alpha = wgt_alpha\n",
    "        self.weight_q = weight_quantization(b=self.w_bit)\n",
    "        #self.register_parameter('wgt_alpha', Parameter(torch.tensor(3.0)))\n",
    "    def forward(self, weight):\n",
    "        #mean = weight.data.mean()\n",
    "        #std = weight.data.std()\n",
    "        #weight = weight.add(-mean).div(std)      # weights normalization\n",
    "        weight_q = self.weight_q(weight, self.wgt_alpha)\n",
    "\n",
    "        return weight_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_alpha=1\n",
    "w_bits=16\n",
    "weight_quant = weight_quantize_fn(w_bit= w_bits)  ## define quant function\n",
    "weight_quant.wgt_alpha = w_alpha\n",
    "fc1_quant      = weight_quant(net6[0].weight)\n",
    "w_delta        = w_alpha/(2**(w_bits-1)-1)\n",
    "fc1_int        = fc1_quant/w_delta\n",
    "print(\"FC1 Weights: \\n\",fc1_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c401c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for layer in net6:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "                layer.weight = Parameter(weight_quant(layer.weight))\n",
    "                w_delta = w_alpha/(2**(w_bits-1)-1)\n",
    "                layer.weight = Parameter(layer.weight/w_delta)\n",
    "                layer.bias = Parameter(layer.bias/w_delta)\n",
    "#                 print(layer.weight)\n",
    "#                 print(layer.bias)\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "                layer.weight = Parameter(weight_quant(layer.weight))\n",
    "                w_delta = w_alpha/(2**(w_bits-1)-1)\n",
    "                layer.weight = Parameter(layer.weight/w_delta)\n",
    "                layer.bias = Parameter(layer.bias/w_delta)\n",
    "#                 print(layer.weight)\n",
    "#                 print(layer.bias)\n",
    "        if isinstance(layer, snn.Leaky):\n",
    "                layer.threshold = layer.threshold/w_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = batch_accuracy(test_loader, net6, num_steps, batch_size)\n",
    "\n",
    "print(f\"The total accuracy on the test set is: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint({'state_dict': net6.state_dict(),}, 1, fdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86841624",
   "metadata": {},
   "source": [
    "### Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45ec61cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=1000, bias=True)\n",
       "  (1): Leaky()\n",
       "  (2): Linear(in_features=1000, out_features=10, bias=True)\n",
       "  (3): Leaky()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path = '/Volumes/export/isn/keli/Desktop/CRI/result/model_quan.pth.tar'\n",
    "checkpoint = torch.load(best_model_path)\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "286d4bd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_accuracy\u001b[49m(test_loader, net, num_steps, batch_size)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe total accuracy on the test set is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "test_acc = batch_accuracy(test_loader, net, num_steps, batch_size)\n",
    "print(f\"The total accuracy on the test set is: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a6820f",
   "metadata": {},
   "source": [
    "## Mapping into CRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b214e90c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 784])\n",
      "torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(net):\n",
    "    if i % 2 == 0:\n",
    "        print(layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30db36b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7424.0\n",
      "5286.0\n"
     ]
    }
   ],
   "source": [
    "# extract weights and bias for torchsnn\n",
    "layers, biases = [], []\n",
    "for i, layer in enumerate(net):\n",
    "    if i % 2 == 0:\n",
    "        layers.append(layer.weight.detach().cpu().numpy())\n",
    "        biases.append(layer.bias.detach().cpu().numpy())\n",
    "\n",
    "print(np.min(layers[1]))\n",
    "print(np.max(layers[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "103031dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784)\n",
      "(1000,)\n",
      "(10, 1000)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "for layerNum, layer in enumerate(layers):\n",
    "    print(layer.shape)\n",
    "    print(biases[layerNum].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf3a6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2dOutputSize(layer,inputSize):\n",
    "    H_out = (inputSize[0] + layer.padding[0]-layer.dilation[0]*(layer.kernel_size[0]-1)-1)/layer.stride[0] +1\n",
    "    W_out = (inputSize[1] + layer.padding[0]-layer.dilation[1]*(layer.kernel_size[1]-1)-1)/layer.stride[1] +1\n",
    "    return [layer.out_channels,int(H_out),int(W_out)]\n",
    "\n",
    "def maxPoolOutputSize(layer,inputSize):\n",
    "    H_out = (inputSize[1] + layer.padding - layer.dilation*(layer.kernel_size-1)-1)/layer.stride +1\n",
    "    W_out = (inputSize[2] + layer.padding - layer.dilation*(layer.kernel_size-1)-1)/layer.stride +1\n",
    "    return [inputSize[0],int(H_out),int(W_out)]\n",
    "\n",
    "def AvgPoolOutputSize(layer,inputSize):\n",
    "    H_out = (inputSize[1] + layer.padding*2 - (layer.kernel_size-1))/layer.stride +1\n",
    "    W_out = (inputSize[2] + layer.padding*2 - (layer.kernel_size-1))/layer.stride +1\n",
    "    return [inputSize[0],int(H_out),int(W_out)]\n",
    "\n",
    "def conv2dToCRI(inputs,output,layer,axonsDict=None,neuronsDict=None):\n",
    "    Hk, Wk = layer.kernel_size\n",
    "    Ho, Wo = output.shape[1],output.shape[2]\n",
    "    pad_top,pad_left = Hk//2,Wk//2\n",
    "    filters = layer.weight.detach().cpu().numpy()\n",
    "    if axonsDict is not None:\n",
    "        Hi, Wi = inputs.shape\n",
    "        for row in range(pad_top,Hi-pad_top):\n",
    "            for col in range(pad_left,Wi-pad_left):\n",
    "                patch = inputs[row-pad_top:row+pad_top+1,col-pad_left:col+pad_left+1]\n",
    "                for filIdx, fil in enumerate(filters):\n",
    "                    postSynapticID = str(output[filIdx,row-pad_top,col-pad_left])\n",
    "                    for i,axons in enumerate(patch):\n",
    "                        for j,axon in enumerate(axons):\n",
    "                            axonsDict[axon].append((postSynapticID,int(fil[0,i,j])))\n",
    "    else:\n",
    "        Hi, Wi = inputs.shape[1],inputs.shape[2]\n",
    "        for channel in range(inputs.shape[0]):\n",
    "            for row in range(pad_top,Hi-pad_top):\n",
    "                for col in range(pad_left,Wi-pad_left):\n",
    "                    patch = inputs[channel,row-pad_top:row+pad_top+1,col-pad_left:col+pad_left+1]\n",
    "                    for filIdx, fil in enumerate(filters):\n",
    "                        postSynapticID = str(output[filIdx,row-pad_top,col-pad_left])\n",
    "                        for i,neurons in enumerate(patch):\n",
    "                            for j,neuron in enumerate(neurons):\n",
    "                                neuronsDict[str(neuron)].append((postSynapticID,int(fil[channel,i,j])))\n",
    "\n",
    "def maxPoolToCRI(inputs,output,layer,neuronsDict):\n",
    "    Hk, Wk = layer.kernel_size, layer.kernel_size\n",
    "    Hi, Wi = inputs.shape[1],inputs.shape[2]\n",
    "    Ho, Wo = output.shape[1],output.shape[2]\n",
    "    pad_top,pad_left = Hk//2,Wk//2\n",
    "    scaler = 1e6\n",
    "    for row in range(0,Hi,2):\n",
    "        for col in range(0,Wi,2):\n",
    "            for channel in range(inputs.shape[0]):\n",
    "                patch = inputs[channel,row:row+pad_top+1,col:col+pad_left+1]\n",
    "                postSynapticID = str(output[channel,row//2,col//2])\n",
    "                for i,preSynNeurons in enumerate(patch):\n",
    "                    for j,preSynNeuron in enumerate(preSynNeurons):\n",
    "                        neuronsDict[str(preSynNeuron)].append((postSynapticID,scaler))\n",
    "                        \n",
    "                        \n",
    "def avgPoolToCRI(inputs,output,layer,neuronsDict):\n",
    "    Hk, Wk = layer.kernel_size, layer.kernel_size\n",
    "    Hi, Wi = inputs.shape[1],inputs.shape[2]\n",
    "    Ho, Wo = output.shape[1],output.shape[2]\n",
    "    pad_top,pad_left = Hk//2,Wk//2\n",
    "    scaler = 1e6\n",
    "    for row in range(0,Hi,2):\n",
    "        for col in range(0,Wi,2):\n",
    "            for channel in range(inputs.shape[0]):\n",
    "                patch = inputs[channel,row:row+pad_top+1,col:col+pad_left+1]\n",
    "                postSynapticID = str(output[channel,row//2,col//2])\n",
    "                for i,preSynNeurons in enumerate(patch):\n",
    "                    for j,preSynNeuron in enumerate(preSynNeurons):\n",
    "                        neuronsDict[str(preSynNeuron)].append((postSynapticID,scaler))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a62ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearToCRI(inputs,output,layer,axonsDict=None,neuronsDict=None,outputNeurons=None):\n",
    "    inputs = inputs.flatten()\n",
    "    weight = layer.weight.detach().cpu().numpy()\n",
    "    if axonsDict is not None:\n",
    "        for baseNeuronIdx, neuron in enumerate(weight.T):\n",
    "            axonID = inputs[baseNeuronIdx]\n",
    "            axonsDict[axonID] = [(str(basePostSynapticID), int(synapseWeight)) for basePostSynapticID, synapseWeight in enumerate(neuron) if synapseWeight != 0]\n",
    "    else:\n",
    "        currLayerNeuronIdxOffset,nextLayerNeuronIdxOffset = inputs[0],inputs[-1]+1\n",
    "        for baseNeuronIdx, neuron in enumerate(weight.T):\n",
    "            neuronID = str(baseNeuronIdx+currLayerNeuronIdxOffset)\n",
    "            neuronEntry = [(str(basePostSynapticID+nextLayerNeuronIdxOffset), int(synapseWeight)) for basePostSynapticID, synapseWeight in enumerate(neuron) if synapseWeight != 0]\n",
    "            neuronsDict[neuronID] = neuronEntry\n",
    "    if outputNeurons is not None:\n",
    "        print('instantiate output neurons')\n",
    "        for baseNeuronIdx in range(layer.out_features):\n",
    "            neuronID = str(baseNeuronIdx+nextLayerNeuronIdxOffset)\n",
    "            neuronsDict[neuronID] = []\n",
    "            outputNeurons.append(neuronID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c87aeda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearBiasAxons(layer,axonsDict,axonOffset,outputs):\n",
    "    biases = layer.bias.detach().cpu().numpy()\n",
    "    for biasIdx, bias in enumerate(biases):\n",
    "        biasID = 'a'+str(biasIdx+axonOffset)\n",
    "        axonsDict[biasID] = [(str(outputs[biasIdx]),int(bias))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af2ed87f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing Axons\n",
      "output layer shape(infeature, outfeature):  [28 28] , 1000\n",
      "constructing bias axons for input layer: 1000 axons\n",
      "constructing output layer\n",
      "output layer shape(infeature, outfeature):  1000 , 10\n",
      "instantiate output neurons\n",
      "constructing bias axons for output linearlayer: 10 axons\n",
      "Numer of neurons: 1010\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "axonsDict = defaultdict(list)\n",
    "neuronsDict = defaultdict(list)\n",
    "outputNeurons = []\n",
    "H_in, W_in = 28, 28\n",
    "inputSize = np.array([H_in, W_in])\n",
    "axonOffset = 0\n",
    "neuronOffset = 0\n",
    "currInput = None\n",
    "\n",
    "for layerIdx, layer in enumerate(net):\n",
    "    if layerIdx == 0: #input layer\n",
    "        if isinstance(layer,torch.nn.Conv2d):\n",
    "            print('constructing Axons')\n",
    "            outputSize = conv2dOutputSize(layer,inputSize)\n",
    "            print(\"Input layer shape(infeature, outfeature): \", inputSize,',',outputSize)\n",
    "            input = np.arange(0,np.prod(inputSize),dtype=int).reshape(inputSize)\n",
    "            inputAxons = np.array([['a'+str(i) for i in row] for row in input])\n",
    "            output = np.arange(0,np.prod(outputSize),dtype=int).reshape(outputSize)\n",
    "            conv2dToCRI(inputAxons,output,layer,axonsDict)\n",
    "            axonOffset += len(axonsDict)\n",
    "            print('constructing bias axons for input layer:',layer.bias.shape[0],'axons')\n",
    "            convBiasAxons(layer,axonsDict,axonOffset,output)\n",
    "            axonOffset += layer.bias.shape[0]\n",
    "            currInput = output\n",
    "        if isinstance(layer,torch.nn.Linear):\n",
    "            print('constructing Axons')\n",
    "            outputSize = layer.out_features\n",
    "            print(\"output layer shape(infeature, outfeature): \", inputSize,',',outputSize)\n",
    "            input = np.arange(0,np.prod(inputSize),dtype=int).reshape(inputSize)\n",
    "            inputAxons = np.array([['a'+str(i) for i in row] for row in input])\n",
    "            output = np.arange(0,outputSize,dtype=int)\n",
    "            linearToCRI(inputAxons,output,layer,axonsDict=axonsDict)\n",
    "            axonOffset += len(axonsDict)\n",
    "            print('constructing bias axons for input layer:',layer.bias.shape[0],'axons')\n",
    "            linearBiasAxons(layer,axonsDict,axonOffset,output)\n",
    "            axonOffset += layer.bias.shape[0]\n",
    "            currInput = output\n",
    "    elif layerIdx == len(net)-2: #output layer\n",
    "        if isinstance(layer,torch.nn.Linear):\n",
    "            print('constructing output layer')\n",
    "            outputSize = layer.out_features\n",
    "            print(\"output layer shape(infeature, outfeature): \", currInput.flatten().shape[0],',',outputSize)\n",
    "            neuronOffset += np.prod(currInput.shape)\n",
    "            output = np.arange(neuronOffset,neuronOffset+outputSize,dtype=int)\n",
    "            linearToCRI(currInput,output,layer,neuronsDict = neuronsDict,outputNeurons=outputNeurons)\n",
    "            print('constructing bias axons for output linearlayer:',layer.bias.shape[0],'axons')\n",
    "            print('Numer of neurons:',len(neuronsDict))\n",
    "            linearBiasAxons(layer,axonsDict,axonOffset,output)\n",
    "            axonOffset += layer.bias.shape[0]\n",
    "    else: #hidden layer\n",
    "        if isinstance(layer,torch.nn.AvgPool2d):\n",
    "            print('constructing hidden avgpool layer')\n",
    "            outputSize = AvgPoolOutputSize(layer,currInput.shape)\n",
    "            print(\"Hidden layer shape(infeature, outfeature): \", currInput.shape,',',outputSize)\n",
    "            neuronOffset += np.prod(currInput.shape)\n",
    "            output = np.arange(neuronOffset,neuronOffset+np.prod(outputSize.shape),dtype=int).reshape(outputSize)\n",
    "            avgPoolToCRI(currInput,output,layer,neuronsDict)\n",
    "            currInput = output\n",
    "            print('Numer of neurons:',len(neuronsDict))\n",
    "        if isinstance(layer,torch.nn.Conv2d):\n",
    "            print('constructing hidden conv2d layer')\n",
    "            outputSize = conv2dOutputSize(layer,currInput.shape)\n",
    "            print(\"Hidden layer shape(infeature, outfeature): \", currInput.shape,',',outputSize)\n",
    "            neuronOffset += np.prod(currInput.shape)\n",
    "            output = np.arange(neuronOffset,neuronOffset+np.prod(outputSize.shape),dtype=int).reshape(outputSize)\n",
    "            conv2dToCRI(currInput,output,layer,neuronsDict=neuronsDict)\n",
    "            print('constructing bias axons for hidden conv2d layer:',layer.bias.shape[0],'axons')\n",
    "            convBiasAxons(layer,axonsDict,axonOffset,output)\n",
    "            axonOffset += layer.bias.shape[0]\n",
    "            currInput = output\n",
    "            print('Numer of neurons:',len(neuronsDict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6dfed23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7687e151",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of axons:  1794\n",
      "Total number of connections between axon and neuron:  784716\n",
      "Max fan out of axon:  1000\n",
      "---\n",
      "Number of neurons:  1010\n",
      "Total number of connections between hidden and output layers:  9997\n",
      "Max fan out of neuron:  10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of axons: \",len(axonsDict))\n",
    "totalAxonSyn = 0\n",
    "maxFan = 0\n",
    "for key in axonsDict.keys():\n",
    "    totalAxonSyn += len(axonsDict[key])\n",
    "    if len(axonsDict[key]) > maxFan:\n",
    "        maxFan = len(axonsDict[key])\n",
    "print(\"Total number of connections between axon and neuron: \", totalAxonSyn)\n",
    "print(\"Max fan out of axon: \", maxFan)\n",
    "print('---')\n",
    "print(\"Number of neurons: \", len(neuronsDict))\n",
    "totalSyn = 0\n",
    "maxFan = 0\n",
    "for key in neuronsDict.keys():\n",
    "    totalSyn += len(neuronsDict[key])\n",
    "    if len(neuronsDict[key]) > maxFan:\n",
    "        maxFan = len(neuronsDict[key])\n",
    "print(\"Total number of connections between hidden and output layers: \", totalSyn)\n",
    "print(\"Max fan out of neuron: \", maxFan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edaa22f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:partitioning library failed to load, multicore disabled\n"
     ]
    }
   ],
   "source": [
    "from l2s.api import CRI_network\n",
    "import cri_simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a46816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added axons to connectome\n",
      "added neurons to connectome\n",
      "added axon synpases\n",
      "added neuron synapses\n",
      "generated Connectome\n",
      "begin: 1\n",
      "end: 1\n",
      "begin: 2\n",
      "end: 2\n",
      "begin: 3\n",
      "end: 3\n",
      "begin: 4\n",
      "end: 4\n",
      "begin: 5\n",
      "end: 5\n",
      "begin: 6\n",
      "end: 6\n",
      "begin: 7\n",
      "end: 7\n",
      "begin: 8\n",
      "end: 8\n",
      "begin: 9\n",
      "end: 9\n",
      "begin: 10\n",
      "end: 10\n",
      "begin: 11\n",
      "end: 11\n",
      "begin: 12\n",
      "end: 12\n",
      "begin: 13\n",
      "end: 13\n",
      "begin: 14\n",
      "end: 14\n",
      "begin: 15\n",
      "end: 15\n",
      "begin: 16\n",
      "end: 16\n",
      "begin: 17\n",
      "end: 17\n",
      "begin: 18\n",
      "end: 18\n",
      "begin: 19\n",
      "end: 19\n",
      "begin: 20\n",
      "end: 20\n",
      "begin: 21\n",
      "end: 21\n",
      "begin: 22\n",
      "end: 22\n",
      "begin: 23\n",
      "end: 23\n",
      "begin: 24\n",
      "end: 24\n",
      "begin: 25\n",
      "end: 25\n",
      "begin: 26\n",
      "end: 26\n",
      "begin: 27\n",
      "end: 27\n",
      "begin: 28\n",
      "end: 28\n",
      "begin: 29\n",
      "end: 29\n",
      "begin: 30\n",
      "end: 30\n",
      "begin: 31\n",
      "end: 31\n",
      "begin: 32\n",
      "end: 32\n",
      "begin: 33\n",
      "end: 33\n",
      "begin: 34\n",
      "end: 34\n",
      "begin: 35\n",
      "end: 35\n",
      "begin: 36\n",
      "end: 36\n",
      "begin: 37\n",
      "end: 37\n",
      "begin: 38\n",
      "end: 38\n",
      "begin: 39\n",
      "end: 39\n",
      "begin: 40\n",
      "end: 40\n",
      "begin: 41\n",
      "end: 41\n",
      "begin: 42\n",
      "end: 42\n",
      "begin: 43\n",
      "end: 43\n",
      "begin: 44\n",
      "end: 44\n",
      "begin: 45\n",
      "end: 45\n",
      "begin: 46\n",
      "end: 46\n",
      "begin: 47\n",
      "end: 47\n",
      "begin: 48\n",
      "end: 48\n",
      "begin: 49\n",
      "end: 49\n",
      "begin: 50\n",
      "end: 50\n",
      "begin: 51\n",
      "end: 51\n",
      "begin: 52\n",
      "end: 52\n",
      "begin: 53\n",
      "end: 53\n",
      "begin: 54\n",
      "end: 54\n",
      "begin: 55\n",
      "end: 55\n",
      "begin: 56\n",
      "end: 56\n",
      "begin: 57\n",
      "end: 57\n",
      "begin: 58\n",
      "end: 58\n",
      "begin: 59\n",
      "end: 59\n",
      "begin: 60\n",
      "end: 60\n",
      "begin: 61\n",
      "end: 61\n",
      "begin: 62\n",
      "end: 62\n",
      "begin: 63\n",
      "end: 63\n",
      "begin: 64\n",
      "end: 64\n",
      "begin: 65\n",
      "end: 65\n",
      "begin: 66\n",
      "end: 66\n",
      "begin: 67\n",
      "end: 67\n",
      "begin: 68\n",
      "end: 68\n",
      "begin: 69\n",
      "end: 69\n",
      "begin: 70\n",
      "end: 70\n",
      "begin: 71\n",
      "end: 71\n",
      "begin: 72\n",
      "end: 72\n",
      "begin: 73\n",
      "end: 73\n",
      "begin: 74\n",
      "end: 74\n",
      "begin: 75\n",
      "end: 75\n",
      "begin: 76\n",
      "end: 76\n",
      "begin: 77\n",
      "end: 77\n",
      "begin: 78\n",
      "end: 78\n",
      "begin: 79\n",
      "end: 79\n",
      "begin: 80\n",
      "end: 80\n",
      "begin: 81\n",
      "end: 81\n",
      "begin: 82\n",
      "end: 82\n",
      "begin: 83\n",
      "end: 83\n",
      "begin: 84\n",
      "end: 84\n",
      "begin: 85\n",
      "end: 85\n",
      "begin: 86\n",
      "end: 86\n",
      "begin: 87\n",
      "end: 87\n",
      "begin: 88\n",
      "end: 88\n",
      "begin: 89\n",
      "end: 89\n",
      "begin: 90\n",
      "end: 90\n",
      "begin: 91\n",
      "end: 91\n",
      "begin: 92\n",
      "end: 92\n",
      "begin: 93\n",
      "end: 93\n",
      "begin: 94\n",
      "end: 94\n",
      "begin: 95\n",
      "end: 95\n",
      "begin: 96\n",
      "end: 96\n",
      "begin: 97\n",
      "end: 97\n",
      "begin: 98\n",
      "end: 98\n",
      "begin: 99\n",
      "end: 99\n",
      "begin: 100\n",
      "end: 100\n",
      "begin: 101\n",
      "end: 101\n",
      "begin: 102\n",
      "end: 102\n",
      "begin: 103\n",
      "end: 103\n",
      "begin: 104\n",
      "end: 104\n",
      "begin: 105\n",
      "end: 105\n",
      "begin: 106\n",
      "end: 106\n",
      "begin: 107\n",
      "end: 107\n",
      "begin: 108\n",
      "end: 108\n",
      "begin: 109\n",
      "end: 109\n",
      "begin: 110\n",
      "end: 110\n",
      "begin: 111\n",
      "end: 111\n",
      "begin: 112\n",
      "end: 112\n",
      "begin: 113\n",
      "end: 113\n",
      "begin: 114\n",
      "end: 114\n",
      "begin: 115\n",
      "end: 115\n",
      "begin: 116\n",
      "end: 116\n",
      "begin: 117\n",
      "end: 117\n",
      "begin: 118\n",
      "end: 118\n",
      "begin: 119\n",
      "end: 119\n",
      "begin: 120\n",
      "end: 120\n",
      "begin: 121\n",
      "end: 121\n",
      "begin: 122\n",
      "end: 122\n",
      "begin: 123\n",
      "end: 123\n",
      "begin: 124\n",
      "end: 124\n",
      "begin: 125\n",
      "end: 125\n",
      "begin: 126\n",
      "end: 126\n",
      "begin: 127\n",
      "end: 127\n",
      "begin: 128\n",
      "end: 128\n",
      "begin: 129\n",
      "end: 129\n",
      "begin: 130\n",
      "end: 130\n",
      "begin: 131\n",
      "end: 131\n",
      "begin: 132\n",
      "end: 132\n",
      "begin: 133\n",
      "end: 133\n",
      "begin: 134\n",
      "end: 134\n",
      "begin: 135\n",
      "end: 135\n",
      "begin: 136\n",
      "end: 136\n",
      "begin: 137\n",
      "end: 137\n",
      "begin: 138\n",
      "end: 138\n",
      "begin: 139\n",
      "end: 139\n",
      "begin: 140\n",
      "end: 140\n",
      "begin: 141\n",
      "end: 141\n",
      "begin: 142\n",
      "end: 142\n",
      "begin: 143\n",
      "end: 143\n",
      "begin: 144\n",
      "end: 144\n",
      "begin: 145\n",
      "end: 145\n",
      "begin: 146\n",
      "end: 146\n",
      "begin: 147\n",
      "end: 147\n",
      "begin: 148\n",
      "end: 148\n",
      "begin: 149\n",
      "end: 149\n",
      "begin: 150\n",
      "end: 150\n",
      "begin: 151\n",
      "end: 151\n",
      "begin: 152\n",
      "end: 152\n",
      "begin: 153\n",
      "end: 153\n",
      "begin: 154\n",
      "end: 154\n",
      "begin: 155\n",
      "end: 155\n",
      "begin: 156\n",
      "end: 156\n",
      "begin: 157\n",
      "end: 157\n",
      "begin: 158\n",
      "end: 158\n",
      "begin: 159\n",
      "end: 159\n",
      "begin: 160\n",
      "end: 160\n",
      "begin: 161\n",
      "end: 161\n",
      "begin: 162\n",
      "end: 162\n",
      "begin: 163\n",
      "end: 163\n",
      "begin: 164\n",
      "end: 164\n",
      "begin: 165\n",
      "end: 165\n",
      "begin: 166\n",
      "end: 166\n",
      "begin: 167\n",
      "end: 167\n",
      "begin: 168\n",
      "end: 168\n",
      "begin: 169\n",
      "end: 169\n",
      "begin: 170\n",
      "end: 170\n",
      "begin: 171\n",
      "end: 171\n",
      "begin: 172\n",
      "end: 172\n",
      "begin: 173\n",
      "end: 173\n",
      "begin: 174\n",
      "end: 174\n",
      "begin: 175\n",
      "end: 175\n",
      "begin: 176\n",
      "end: 176\n",
      "begin: 177\n",
      "end: 177\n",
      "begin: 178\n",
      "end: 178\n",
      "begin: 179\n",
      "end: 179\n",
      "begin: 180\n",
      "end: 180\n",
      "begin: 181\n",
      "end: 181\n",
      "begin: 182\n",
      "end: 182\n",
      "begin: 183\n",
      "end: 183\n",
      "begin: 184\n",
      "end: 184\n",
      "begin: 185\n",
      "end: 185\n",
      "begin: 186\n",
      "end: 186\n",
      "begin: 187\n",
      "end: 187\n",
      "begin: 188\n",
      "end: 188\n",
      "begin: 189\n",
      "end: 189\n",
      "begin: 190\n",
      "end: 190\n",
      "begin: 191\n",
      "end: 191\n",
      "begin: 192\n",
      "end: 192\n",
      "begin: 193\n",
      "end: 193\n",
      "begin: 194\n",
      "end: 194\n",
      "begin: 195\n",
      "end: 195\n",
      "begin: 196\n",
      "end: 196\n",
      "begin: 197\n",
      "end: 197\n",
      "begin: 198\n",
      "end: 198\n",
      "begin: 199\n",
      "end: 199\n",
      "begin: 200\n",
      "end: 200\n",
      "begin: 201\n",
      "end: 201\n",
      "begin: 202\n",
      "end: 202\n",
      "begin: 203\n",
      "end: 203\n",
      "begin: 204\n",
      "end: 204\n",
      "begin: 205\n",
      "end: 205\n",
      "begin: 206\n",
      "end: 206\n",
      "begin: 207\n",
      "end: 207\n",
      "begin: 208\n",
      "end: 208\n",
      "begin: 209\n",
      "end: 209\n",
      "begin: 210\n",
      "end: 210\n",
      "begin: 211\n",
      "end: 211\n",
      "begin: 212\n",
      "end: 212\n",
      "begin: 213\n",
      "end: 213\n",
      "begin: 214\n",
      "end: 214\n",
      "begin: 215\n",
      "end: 215\n",
      "begin: 216\n",
      "end: 216\n",
      "begin: 217\n",
      "end: 217\n",
      "begin: 218\n",
      "end: 218\n",
      "begin: 219\n",
      "end: 219\n",
      "begin: 220\n",
      "end: 220\n",
      "begin: 221\n",
      "end: 221\n",
      "begin: 222\n",
      "end: 222\n",
      "begin: 223\n",
      "end: 223\n",
      "begin: 224\n",
      "end: 224\n",
      "begin: 225\n",
      "end: 225\n",
      "begin: 226\n",
      "end: 226\n",
      "begin: 227\n",
      "end: 227\n",
      "begin: 228\n",
      "end: 228\n",
      "begin: 229\n",
      "end: 229\n",
      "begin: 230\n",
      "end: 230\n",
      "begin: 231\n",
      "end: 231\n",
      "begin: 232\n",
      "end: 232\n",
      "begin: 233\n",
      "end: 233\n",
      "begin: 234\n",
      "end: 234\n",
      "begin: 235\n",
      "end: 235\n",
      "begin: 236\n",
      "end: 236\n",
      "begin: 237\n",
      "end: 237\n",
      "begin: 238\n",
      "end: 238\n",
      "begin: 239\n",
      "end: 239\n",
      "begin: 240\n",
      "end: 240\n",
      "begin: 241\n",
      "end: 241\n",
      "begin: 242\n",
      "end: 242\n",
      "begin: 243\n",
      "end: 243\n",
      "begin: 244\n",
      "end: 244\n",
      "begin: 245\n",
      "end: 245\n",
      "begin: 246\n",
      "end: 246\n",
      "begin: 247\n",
      "end: 247\n",
      "begin: 248\n",
      "end: 248\n",
      "begin: 249\n",
      "end: 249\n",
      "begin: 250\n",
      "end: 250\n",
      "begin: 251\n",
      "end: 251\n",
      "begin: 252\n",
      "end: 252\n",
      "begin: 253\n",
      "end: 253\n",
      "begin: 254\n",
      "end: 254\n",
      "begin: 255\n",
      "end: 255\n",
      "begin: 256\n",
      "end: 256\n",
      "begin: 257\n",
      "end: 257\n",
      "begin: 258\n",
      "end: 258\n",
      "begin: 259\n",
      "end: 259\n",
      "begin: 260\n",
      "end: 260\n",
      "begin: 261\n",
      "end: 261\n",
      "begin: 262\n",
      "end: 262\n",
      "begin: 263\n",
      "end: 263\n",
      "begin: 264\n",
      "end: 264\n",
      "begin: 265\n",
      "end: 265\n",
      "begin: 266\n",
      "end: 266\n",
      "begin: 267\n",
      "end: 267\n",
      "begin: 268\n",
      "end: 268\n",
      "begin: 269\n",
      "end: 269\n",
      "begin: 270\n",
      "end: 270\n",
      "begin: 271\n",
      "end: 271\n",
      "begin: 272\n",
      "end: 272\n",
      "begin: 273\n",
      "end: 273\n",
      "begin: 274\n",
      "end: 274\n",
      "begin: 275\n",
      "end: 275\n",
      "begin: 276\n",
      "end: 276\n",
      "begin: 277\n",
      "end: 277\n",
      "begin: 278\n",
      "end: 278\n",
      "begin: 279\n",
      "end: 279\n",
      "begin: 280\n",
      "end: 280\n",
      "begin: 281\n",
      "end: 281\n",
      "begin: 282\n",
      "end: 282\n",
      "begin: 283\n",
      "end: 283\n",
      "begin: 284\n",
      "end: 284\n",
      "begin: 285\n",
      "end: 285\n",
      "begin: 286\n",
      "end: 286\n",
      "begin: 287\n",
      "end: 287\n",
      "begin: 288\n",
      "end: 288\n",
      "begin: 289\n",
      "end: 289\n",
      "begin: 290\n",
      "end: 290\n",
      "begin: 291\n",
      "end: 291\n",
      "begin: 292\n",
      "end: 292\n",
      "begin: 293\n",
      "end: 293\n",
      "begin: 294\n",
      "end: 294\n",
      "begin: 295\n",
      "end: 295\n",
      "begin: 296\n",
      "end: 296\n",
      "begin: 297\n",
      "end: 297\n",
      "begin: 298\n",
      "end: 298\n",
      "begin: 299\n",
      "end: 299\n",
      "begin: 300\n",
      "end: 300\n",
      "begin: 301\n",
      "end: 301\n",
      "begin: 302\n",
      "end: 302\n",
      "begin: 303\n",
      "end: 303\n",
      "begin: 304\n",
      "end: 304\n",
      "begin: 305\n",
      "end: 305\n",
      "begin: 306\n",
      "end: 306\n",
      "begin: 307\n",
      "end: 307\n",
      "begin: 308\n",
      "end: 308\n",
      "begin: 309\n",
      "end: 309\n",
      "begin: 310\n",
      "end: 310\n",
      "begin: 311\n",
      "end: 311\n",
      "begin: 312\n",
      "end: 312\n",
      "begin: 313\n",
      "end: 313\n",
      "begin: 314\n",
      "end: 314\n",
      "begin: 315\n",
      "end: 315\n",
      "begin: 316\n",
      "end: 316\n",
      "begin: 317\n",
      "end: 317\n",
      "begin: 318\n",
      "end: 318\n",
      "begin: 319\n",
      "end: 319\n",
      "begin: 320\n",
      "end: 320\n",
      "begin: 321\n",
      "end: 321\n",
      "begin: 322\n",
      "end: 322\n",
      "begin: 323\n",
      "end: 323\n",
      "begin: 324\n",
      "end: 324\n",
      "begin: 325\n",
      "end: 325\n",
      "begin: 326\n",
      "end: 326\n",
      "begin: 327\n",
      "end: 327\n",
      "begin: 328\n",
      "end: 328\n",
      "begin: 329\n",
      "end: 329\n",
      "begin: 330\n",
      "end: 330\n",
      "begin: 331\n",
      "end: 331\n",
      "begin: 332\n",
      "end: 332\n",
      "begin: 333\n",
      "end: 333\n",
      "begin: 334\n",
      "end: 334\n",
      "begin: 335\n",
      "end: 335\n",
      "begin: 336\n",
      "end: 336\n",
      "begin: 337\n",
      "end: 337\n",
      "begin: 338\n",
      "end: 338\n",
      "begin: 339\n",
      "end: 339\n",
      "begin: 340\n",
      "end: 340\n",
      "begin: 341\n",
      "end: 341\n",
      "begin: 342\n",
      "end: 342\n",
      "begin: 343\n",
      "end: 343\n",
      "begin: 344\n",
      "end: 344\n",
      "begin: 345\n",
      "end: 345\n",
      "begin: 346\n",
      "end: 346\n",
      "begin: 347\n",
      "end: 347\n",
      "begin: 348\n",
      "end: 348\n",
      "begin: 349\n",
      "end: 349\n",
      "begin: 350\n",
      "end: 350\n",
      "begin: 351\n",
      "end: 351\n",
      "begin: 352\n",
      "end: 352\n",
      "begin: 353\n",
      "end: 353\n",
      "begin: 354\n",
      "end: 354\n",
      "begin: 355\n",
      "end: 355\n",
      "begin: 356\n",
      "end: 356\n",
      "begin: 357\n",
      "end: 357\n",
      "begin: 358\n",
      "end: 358\n",
      "begin: 359\n",
      "end: 359\n",
      "begin: 360\n",
      "end: 360\n",
      "begin: 361\n",
      "end: 361\n",
      "begin: 362\n",
      "end: 362\n",
      "begin: 363\n",
      "end: 363\n",
      "begin: 364\n",
      "end: 364\n",
      "begin: 365\n",
      "end: 365\n",
      "begin: 366\n",
      "end: 366\n",
      "begin: 367\n",
      "end: 367\n",
      "begin: 368\n",
      "end: 368\n",
      "begin: 369\n",
      "end: 369\n",
      "begin: 370\n",
      "end: 370\n",
      "begin: 371\n",
      "end: 371\n",
      "begin: 372\n",
      "end: 372\n",
      "begin: 373\n",
      "end: 373\n",
      "begin: 374\n",
      "end: 374\n",
      "begin: 375\n",
      "end: 375\n",
      "begin: 376\n",
      "end: 376\n",
      "begin: 377\n",
      "end: 377\n",
      "begin: 378\n",
      "end: 378\n",
      "begin: 379\n",
      "end: 379\n",
      "begin: 380\n",
      "end: 380\n",
      "begin: 381\n",
      "end: 381\n",
      "begin: 382\n",
      "end: 382\n",
      "begin: 383\n",
      "end: 383\n",
      "begin: 384\n",
      "end: 384\n",
      "begin: 385\n",
      "end: 385\n",
      "begin: 386\n",
      "end: 386\n",
      "begin: 387\n",
      "end: 387\n",
      "begin: 388\n",
      "end: 388\n",
      "begin: 389\n",
      "end: 389\n",
      "begin: 390\n",
      "end: 390\n",
      "begin: 391\n",
      "end: 391\n",
      "begin: 392\n",
      "end: 392\n",
      "begin: 393\n",
      "end: 393\n",
      "begin: 394\n",
      "end: 394\n",
      "begin: 395\n",
      "end: 395\n",
      "begin: 396\n",
      "end: 396\n",
      "begin: 397\n",
      "end: 397\n",
      "begin: 398\n",
      "end: 398\n",
      "begin: 399\n",
      "end: 399\n",
      "begin: 400\n",
      "end: 400\n",
      "begin: 401\n",
      "end: 401\n",
      "begin: 402\n",
      "end: 402\n",
      "begin: 403\n",
      "end: 403\n",
      "begin: 404\n",
      "end: 404\n",
      "begin: 405\n",
      "end: 405\n",
      "begin: 406\n",
      "end: 406\n",
      "begin: 407\n",
      "end: 407\n",
      "begin: 408\n",
      "end: 408\n",
      "begin: 409\n",
      "end: 409\n",
      "begin: 410\n",
      "end: 410\n",
      "begin: 411\n",
      "end: 411\n",
      "begin: 412\n",
      "end: 412\n",
      "begin: 413\n",
      "end: 413\n",
      "begin: 414\n",
      "end: 414\n",
      "begin: 415\n",
      "end: 415\n",
      "begin: 416\n",
      "end: 416\n",
      "begin: 417\n",
      "end: 417\n",
      "begin: 418\n",
      "end: 418\n",
      "begin: 419\n",
      "end: 419\n",
      "begin: 420\n",
      "end: 420\n",
      "begin: 421\n",
      "end: 421\n",
      "begin: 422\n",
      "end: 422\n",
      "begin: 423\n",
      "end: 423\n",
      "begin: 424\n",
      "end: 424\n",
      "begin: 425\n",
      "end: 425\n",
      "begin: 426\n",
      "end: 426\n",
      "begin: 427\n",
      "end: 427\n",
      "begin: 428\n",
      "end: 428\n",
      "begin: 429\n",
      "end: 429\n",
      "begin: 430\n",
      "end: 430\n",
      "begin: 431\n",
      "end: 431\n",
      "begin: 432\n",
      "end: 432\n",
      "begin: 433\n",
      "end: 433\n",
      "begin: 434\n",
      "end: 434\n",
      "begin: 435\n",
      "end: 435\n",
      "begin: 436\n",
      "end: 436\n",
      "begin: 437\n",
      "end: 437\n",
      "begin: 438\n",
      "end: 438\n",
      "begin: 439\n",
      "end: 439\n",
      "begin: 440\n",
      "end: 440\n",
      "begin: 441\n",
      "end: 441\n",
      "begin: 442\n",
      "end: 442\n",
      "begin: 443\n",
      "end: 443\n",
      "begin: 444\n",
      "end: 444\n",
      "begin: 445\n",
      "end: 445\n",
      "begin: 446\n",
      "end: 446\n",
      "begin: 447\n",
      "end: 447\n",
      "begin: 448\n",
      "end: 448\n",
      "begin: 449\n",
      "end: 449\n",
      "begin: 450\n",
      "end: 450\n",
      "begin: 451\n",
      "end: 451\n",
      "begin: 452\n",
      "end: 452\n",
      "begin: 453\n",
      "end: 453\n",
      "begin: 454\n",
      "end: 454\n",
      "begin: 455\n",
      "end: 455\n",
      "begin: 456\n",
      "end: 456\n",
      "begin: 457\n",
      "end: 457\n",
      "begin: 458\n",
      "end: 458\n",
      "begin: 459\n",
      "end: 459\n",
      "begin: 460\n",
      "end: 460\n",
      "begin: 461\n",
      "end: 461\n",
      "begin: 462\n",
      "end: 462\n",
      "begin: 463\n",
      "end: 463\n",
      "begin: 464\n",
      "end: 464\n",
      "begin: 465\n",
      "end: 465\n",
      "begin: 466\n",
      "end: 466\n",
      "begin: 467\n",
      "end: 467\n",
      "begin: 468\n",
      "end: 468\n",
      "begin: 469\n",
      "end: 469\n",
      "begin: 470\n",
      "end: 470\n",
      "begin: 471\n",
      "end: 471\n",
      "begin: 472\n",
      "end: 472\n",
      "begin: 473\n",
      "end: 473\n",
      "begin: 474\n",
      "end: 474\n",
      "begin: 475\n",
      "end: 475\n",
      "begin: 476\n",
      "end: 476\n",
      "begin: 477\n",
      "end: 477\n",
      "begin: 478\n",
      "end: 478\n",
      "begin: 479\n",
      "end: 479\n",
      "begin: 480\n",
      "end: 480\n",
      "begin: 481\n",
      "end: 481\n",
      "begin: 482\n",
      "end: 482\n",
      "begin: 483\n",
      "end: 483\n",
      "begin: 484\n",
      "end: 484\n",
      "begin: 485\n",
      "end: 485\n",
      "begin: 486\n",
      "end: 486\n",
      "begin: 487\n",
      "end: 487\n",
      "begin: 488\n",
      "end: 488\n",
      "begin: 489\n",
      "end: 489\n",
      "begin: 490\n",
      "end: 490\n",
      "begin: 491\n",
      "end: 491\n",
      "begin: 492\n",
      "end: 492\n",
      "begin: 493\n",
      "end: 493\n",
      "begin: 494\n",
      "end: 494\n",
      "begin: 495\n",
      "end: 495\n",
      "begin: 496\n",
      "end: 496\n",
      "begin: 497\n",
      "end: 497\n",
      "begin: 498\n",
      "end: 498\n",
      "begin: 499\n",
      "end: 499\n",
      "begin: 500\n",
      "end: 500\n",
      "begin: 501\n",
      "end: 501\n",
      "begin: 502\n",
      "end: 502\n",
      "begin: 503\n",
      "end: 503\n",
      "begin: 504\n",
      "end: 504\n",
      "begin: 505\n",
      "end: 505\n",
      "begin: 506\n",
      "end: 506\n",
      "begin: 507\n",
      "end: 507\n",
      "begin: 508\n",
      "end: 508\n",
      "begin: 509\n",
      "end: 509\n",
      "begin: 510\n",
      "end: 510\n",
      "begin: 511\n",
      "end: 511\n",
      "begin: 512\n",
      "end: 512\n",
      "begin: 513\n",
      "end: 513\n",
      "begin: 514\n",
      "end: 514\n",
      "begin: 515\n",
      "end: 515\n",
      "begin: 516\n",
      "end: 516\n",
      "begin: 517\n",
      "end: 517\n",
      "begin: 518\n",
      "end: 518\n",
      "begin: 519\n",
      "end: 519\n",
      "begin: 520\n",
      "end: 520\n",
      "begin: 521\n",
      "end: 521\n",
      "begin: 522\n",
      "end: 522\n",
      "begin: 523\n",
      "end: 523\n",
      "begin: 524\n",
      "end: 524\n",
      "begin: 525\n",
      "end: 525\n",
      "begin: 526\n",
      "end: 526\n",
      "begin: 527\n",
      "end: 527\n",
      "begin: 528\n",
      "end: 528\n",
      "begin: 529\n",
      "end: 529\n",
      "begin: 530\n",
      "end: 530\n",
      "begin: 531\n",
      "end: 531\n",
      "begin: 532\n",
      "end: 532\n",
      "begin: 533\n",
      "end: 533\n",
      "begin: 534\n",
      "end: 534\n",
      "begin: 535\n",
      "end: 535\n",
      "begin: 536\n",
      "end: 536\n",
      "begin: 537\n",
      "end: 537\n",
      "begin: 538\n",
      "end: 538\n",
      "begin: 539\n",
      "end: 539\n",
      "begin: 540\n",
      "end: 540\n",
      "begin: 541\n",
      "end: 541\n",
      "begin: 542\n",
      "end: 542\n",
      "begin: 543\n",
      "end: 543\n",
      "begin: 544\n",
      "end: 544\n",
      "begin: 545\n",
      "end: 545\n",
      "begin: 546\n",
      "end: 546\n",
      "begin: 547\n",
      "end: 547\n",
      "begin: 548\n",
      "end: 548\n",
      "begin: 549\n",
      "end: 549\n",
      "begin: 550\n",
      "end: 550\n",
      "begin: 551\n",
      "end: 551\n",
      "begin: 552\n",
      "end: 552\n",
      "begin: 553\n",
      "end: 553\n",
      "begin: 554\n",
      "end: 554\n",
      "begin: 555\n",
      "end: 555\n",
      "begin: 556\n",
      "end: 556\n",
      "begin: 557\n",
      "end: 557\n",
      "begin: 558\n",
      "end: 558\n",
      "begin: 559\n",
      "end: 559\n",
      "begin: 560\n",
      "end: 560\n",
      "begin: 561\n",
      "end: 561\n",
      "begin: 562\n",
      "end: 562\n",
      "begin: 563\n",
      "end: 563\n",
      "begin: 564\n",
      "end: 564\n",
      "begin: 565\n",
      "end: 565\n",
      "begin: 566\n",
      "end: 566\n",
      "begin: 567\n",
      "end: 567\n",
      "begin: 568\n",
      "end: 568\n",
      "begin: 569\n",
      "end: 569\n",
      "begin: 570\n",
      "end: 570\n",
      "begin: 571\n",
      "end: 571\n",
      "begin: 572\n",
      "end: 572\n",
      "begin: 573\n",
      "end: 573\n",
      "begin: 574\n",
      "end: 574\n",
      "begin: 575\n",
      "end: 575\n",
      "begin: 576\n",
      "end: 576\n",
      "begin: 577\n",
      "end: 577\n",
      "begin: 578\n",
      "end: 578\n",
      "begin: 579\n",
      "end: 579\n",
      "begin: 580\n",
      "end: 580\n",
      "begin: 581\n",
      "end: 581\n",
      "begin: 582\n",
      "end: 582\n",
      "begin: 583\n",
      "end: 583\n",
      "begin: 584\n",
      "end: 584\n",
      "begin: 585\n",
      "end: 585\n",
      "begin: 586\n",
      "end: 586\n",
      "begin: 587\n",
      "end: 587\n",
      "begin: 588\n",
      "end: 588\n",
      "begin: 589\n",
      "end: 589\n",
      "begin: 590\n",
      "end: 590\n",
      "begin: 591\n",
      "end: 591\n",
      "begin: 592\n",
      "end: 592\n",
      "begin: 593\n",
      "end: 593\n",
      "begin: 594\n",
      "end: 594\n",
      "begin: 595\n",
      "end: 595\n",
      "begin: 596\n",
      "end: 596\n",
      "begin: 597\n",
      "end: 597\n",
      "begin: 598\n",
      "end: 598\n",
      "begin: 599\n",
      "end: 599\n",
      "begin: 600\n",
      "end: 600\n",
      "begin: 601\n",
      "end: 601\n",
      "begin: 602\n",
      "end: 602\n",
      "begin: 603\n",
      "end: 603\n",
      "begin: 604\n",
      "end: 604\n",
      "begin: 605\n",
      "end: 605\n",
      "begin: 606\n",
      "end: 606\n",
      "begin: 607\n",
      "end: 607\n",
      "begin: 608\n",
      "end: 608\n",
      "begin: 609\n",
      "end: 609\n",
      "begin: 610\n",
      "end: 610\n",
      "begin: 611\n",
      "end: 611\n",
      "begin: 612\n",
      "end: 612\n",
      "begin: 613\n",
      "end: 613\n",
      "begin: 614\n",
      "end: 614\n",
      "begin: 615\n",
      "end: 615\n",
      "begin: 616\n",
      "end: 616\n",
      "begin: 617\n",
      "end: 617\n",
      "begin: 618\n",
      "end: 618\n",
      "begin: 619\n",
      "end: 619\n",
      "begin: 620\n",
      "end: 620\n",
      "begin: 621\n",
      "end: 621\n",
      "begin: 622\n",
      "end: 622\n",
      "begin: 623\n",
      "end: 623\n",
      "begin: 624\n",
      "end: 624\n",
      "begin: 625\n",
      "end: 625\n",
      "begin: 626\n",
      "end: 626\n",
      "begin: 627\n",
      "end: 627\n",
      "begin: 628\n",
      "end: 628\n",
      "begin: 629\n",
      "end: 629\n",
      "begin: 630\n",
      "end: 630\n",
      "begin: 631\n",
      "end: 631\n",
      "begin: 632\n",
      "end: 632\n",
      "begin: 633\n",
      "end: 633\n",
      "begin: 634\n",
      "end: 634\n",
      "begin: 635\n",
      "end: 635\n",
      "begin: 636\n",
      "end: 636\n",
      "begin: 637\n",
      "end: 637\n",
      "begin: 638\n",
      "end: 638\n",
      "begin: 639\n",
      "end: 639\n",
      "begin: 640\n",
      "end: 640\n",
      "begin: 641\n",
      "end: 641\n",
      "begin: 642\n",
      "end: 642\n",
      "begin: 643\n",
      "end: 643\n",
      "begin: 644\n",
      "end: 644\n",
      "begin: 645\n",
      "end: 645\n",
      "begin: 646\n",
      "end: 646\n",
      "begin: 647\n",
      "end: 647\n",
      "begin: 648\n",
      "end: 648\n",
      "begin: 649\n",
      "end: 649\n",
      "begin: 650\n",
      "end: 650\n",
      "begin: 651\n",
      "end: 651\n",
      "begin: 652\n",
      "end: 652\n",
      "begin: 653\n",
      "end: 653\n",
      "begin: 654\n",
      "end: 654\n",
      "begin: 655\n",
      "end: 655\n",
      "begin: 656\n",
      "end: 656\n",
      "begin: 657\n",
      "end: 657\n",
      "begin: 658\n",
      "end: 658\n",
      "begin: 659\n",
      "end: 659\n",
      "begin: 660\n",
      "end: 660\n",
      "begin: 661\n",
      "end: 661\n",
      "begin: 662\n",
      "end: 662\n",
      "begin: 663\n",
      "end: 663\n",
      "begin: 664\n",
      "end: 664\n",
      "begin: 665\n",
      "end: 665\n",
      "begin: 666\n",
      "end: 666\n",
      "begin: 667\n",
      "end: 667\n",
      "begin: 668\n",
      "end: 668\n",
      "begin: 669\n",
      "end: 669\n",
      "begin: 670\n",
      "end: 670\n",
      "begin: 671\n",
      "end: 671\n",
      "begin: 672\n",
      "end: 672\n",
      "begin: 673\n",
      "end: 673\n",
      "begin: 674\n",
      "end: 674\n",
      "begin: 675\n",
      "end: 675\n",
      "begin: 676\n",
      "end: 676\n",
      "begin: 677\n",
      "end: 677\n",
      "begin: 678\n",
      "end: 678\n",
      "begin: 679\n",
      "end: 679\n",
      "begin: 680\n",
      "end: 680\n",
      "begin: 681\n",
      "end: 681\n",
      "begin: 682\n",
      "end: 682\n",
      "begin: 683\n",
      "end: 683\n",
      "begin: 684\n",
      "end: 684\n",
      "begin: 685\n",
      "end: 685\n",
      "begin: 686\n",
      "end: 686\n",
      "begin: 687\n",
      "end: 687\n",
      "begin: 688\n",
      "end: 688\n",
      "begin: 689\n",
      "end: 689\n",
      "begin: 690\n",
      "end: 690\n",
      "begin: 691\n",
      "end: 691\n",
      "begin: 692\n",
      "end: 692\n",
      "begin: 693\n",
      "end: 693\n",
      "begin: 694\n",
      "end: 694\n",
      "begin: 695\n",
      "end: 695\n",
      "begin: 696\n",
      "end: 696\n",
      "begin: 697\n",
      "end: 697\n",
      "begin: 698\n",
      "end: 698\n",
      "begin: 699\n",
      "end: 699\n",
      "begin: 700\n",
      "end: 700\n",
      "begin: 701\n",
      "end: 701\n",
      "begin: 702\n",
      "end: 702\n",
      "begin: 703\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "config['neuron_type'] = \"I&F\"\n",
    "config['global_neuron_params'] = {}\n",
    "config['global_neuron_params']['v_thr'] = 9*10**4\n",
    "#softwareNetwork = CRI_network(axons=axonsDict,connections=neuronsDict,config=config,target='simpleSim', outputs = outputs)\n",
    "hardwareNetwork = CRI_network(axons=axonsDict,connections=neuronsDict,config=config,target='CRI', outputs = outputs,simDump = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_to_CRI(currentInput):\n",
    "    num_steps = 10\n",
    "    currentInput = currentInput.view(currentInput.size(0), -1)\n",
    "    batch = []\n",
    "    for element in currentInput:\n",
    "        timesteps = []\n",
    "        rateEnc = spikegen.rate(element,num_steps)\n",
    "        rateEnc = rateEnc.detach().cpu().numpy()\n",
    "        for element in rateEnc:\n",
    "            currInput = ['a'+str(idx) for idx,axon in enumerate(element) if axon != 0]\n",
    "            biasInput = ['a'+str(idx) for idx in range(784,len(axonsDict))]\n",
    "#             timesteps.append(currInput)\n",
    "#             timesteps.append(biasInput)\n",
    "            timesteps.append(currInput+biasInput)\n",
    "        batch.append(timesteps)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf4920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CRI(inputList,output_offset):\n",
    "    predictions = []\n",
    "    total_time_cri = 0\n",
    "    #each image\n",
    "    for currInput in inputList:\n",
    "        #reset the membrane potential to zero\n",
    "        softwareNetwork.simpleSim.initialize_sim_vars(len(neuronsDict))\n",
    "        spikeRate = [0]*10\n",
    "        #each time step\n",
    "        for slice in currInput:\n",
    "            start_time = time.time()\n",
    "            swSpike = softwareNetwork.step(slice, membranePotential=False)\n",
    "            end_time = time.time()\n",
    "            total_time_cri = total_time_cri + end_time-start_time\n",
    "            for spike in swSpike:\n",
    "                spikeIdx = int(spike) - output_offset \n",
    "                try: \n",
    "                    if spikeIdx >= 0: \n",
    "                        spikeRate[spikeIdx] += 1 \n",
    "                except:\n",
    "                    print(\"SpikeIdx: \", spikeIdx,\"\\n SpikeRate:\",spikeRate )\n",
    "        predictions.append(spikeRate.index(max(spikeRate)))\n",
    "    print(f\"Total simulation execution time: {total_time_cri:.5f} s\")\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_CRI_hw(inputList,output_offset):\n",
    "    predictions = []\n",
    "    #each image\n",
    "    total_time_cri = 0\n",
    "    for currInput in inputList:\n",
    "        #initiate the softwareNetwork for each image\n",
    "        cri_simulations.FPGA_Execution.fpga_controller.clear(len(neuronsDict), False, 0)  ##Num_neurons, simDump, coreOverride\n",
    "        spikeRate = [0]*10\n",
    "        #each time step\n",
    "        for slice in currInput:\n",
    "            start_time = time.time()\n",
    "            hwSpike = hardwareNetwork.step(slice, membranePotential=False)\n",
    "#             print(\"Mem:\",mem)\n",
    "            end_time = time.time()\n",
    "            total_time_cri = total_time_cri + end_time-start_time\n",
    "            print(hwSpike)\n",
    "            for spike in hwSpike:\n",
    "                print(int(spike))\n",
    "                spikeIdx = int(spike) - output_offset \n",
    "                if spikeIdx >= 0: \n",
    "                    spikeRate[spikeIdx] += 1 \n",
    "        predictions.append(spikeRate.index(max(spikeRate))) \n",
    "    print(f\"Total execution time CRIFPGA: {total_time_cri:.5f} s\")\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78aec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:non-spike packet encountered during spike flush: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 205 171 205 171]\n",
      "ERROR:root:non-spike packet encountered during spike flush: [227 242 130  22   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 186 202 186 202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:non-spike packet encountered during spike flush: [  3  55   8  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 186 186 186 186]\n",
      "ERROR:root:non-spike packet encountered during spike flush: [181  95 131  22   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 186 202 186 202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:non-spike packet encountered during spike flush: [221 161   8  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 186 186 186 186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:non-spike packet encountered during spike flush: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 205 171 205 171]\n",
      "ERROR:root:non-spike packet encountered during spike flush: [172  18   9  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 186 186 186 186]\n",
      "ERROR:root:non-spike packet encountered during spike flush: [197  54 132  22   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 186 202 186 202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:non-spike packet encountered during spike flush: [233 130   9  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 186 186 186 186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:non-spike packet encountered during spike flush: [145 242   9  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 186 186 186 186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:non-spike packet encountered during spike flush: [ 99 106  10  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 186 186 186 186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:non-spike packet encountered during spike flush: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 205 171 205 171]\n",
      "ERROR:root:non-spike packet encountered during spike flush: [113 231  10  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 186 186 186 186]\n",
      "ERROR:root:non-spike packet encountered during spike flush: [145   2 134  22   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 186 202 186 202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:non-spike packet encountered during spike flush: [191 102  11  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 186 186 186 186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:non-spike packet encountered during spike flush: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 205 171 205 171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "cri_correct = 0\n",
    "cri_correct_hw = 0\n",
    "batch_size = 128\n",
    "# drop_last switched to False to keep all samples\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "output_offset = int(outputs[0])\n",
    "with torch.no_grad():\n",
    "    net6.eval()\n",
    "    \n",
    "    train_loader = iter(train_loader)\n",
    "    count = 0\n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        input = input_to_CRI(data)\n",
    "#         criPred = torch.tensor(run_CRI(input,output_offset)).to(device)\n",
    "        criPred_hw = torch.tensor(run_CRI_hw(input,output_offset)).to(device)\n",
    "\n",
    "        # calculate total accuracy\n",
    "        spk_rec, _ = forward_pass(net6, num_steps, data, batch_size)\n",
    "\n",
    "        correct += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "        total += spk_rec.size(1)\n",
    "#         cri_correct += (criPred == targets).sum().item()\n",
    "        cri_correct_hw += (criPred_hw == targets).sum().item()\n",
    "        count += 1\n",
    "#         if count == 12:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a784348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Totoal execution time: {end_time-start_time:.2f} s\")\n",
    "print(f\"Total correctly classified test set images for TorchSNN: {correct}/{total}\")\n",
    "print(f\"Total correctly classified test set images for CRI: {cri_correct}/{total}\")\n",
    "print(f\"Test Set Accuracy for TorchSNN: {100 * correct / total:.2f}%\")\n",
    "print(f\"Test Set Accuracy for CRI: {100 * cri_correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349797c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "782c4fc05a7b0c5006502edc276c124083adbfff5066531c0f613c007bf9a5ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
