{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f72513-7f22-4671-ada2-3778f03199d0",
   "metadata": {},
   "source": [
    "## CRI MNIST Demonstration with Spikingjelly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e707ae-688c-4e1b-82a7-2d0ef221b406",
   "metadata": {},
   "source": [
    "## Training SNN with sikingjelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9b5788-692e-4544-b1bb-641a638fd9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from spikingjelly.activation_based import neuron, functional, surrogate, layer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from torch.cuda import amp\n",
    "import sys\n",
    "import datetime\n",
    "from spikingjelly import visualizing\n",
    "import shutil\n",
    "from quant_layer import *\n",
    "from cri_converter import BN_Folder, Quantize_Network, CRI_Converter\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ef8014-a79f-4067-aefb-f2ba0d0fd4a3",
   "metadata": {},
   "source": [
    "### Import MNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6719a69-eec4-4155-a0d5-d8ccf04c1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader arguments\n",
    "batch_size = 1\n",
    "data_path='~/justinData/mnist'\n",
    "out_dir = 'runs/transformers'\n",
    "epochs = 25\n",
    "start_epoch = 0\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "T = 4\n",
    "channels = 8\n",
    "max_test_acc = -1\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7619cd38-e83b-480b-947d-6dcb7e65485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4b44f-f800-489d-ac6a-f3966ed08711",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beace761-b8e9-4f7e-a685-0dda4d790f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSNN(nn.Module):\n",
    "    def __init__(self, T: int, channels: int, use_cupy=False):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "\n",
    "        self.conv_fc = nn.Sequential(\n",
    "        layer.Conv2d(1, channels, kernel_size=3, padding=1, bias=False),\n",
    "        layer.BatchNorm2d(channels),\n",
    "        neuron.IFNode(surrogate_function=surrogate.ATan()),\n",
    "        layer.AvgPool2d(2, 2),  # 14 * 14\n",
    "\n",
    "        layer.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "        layer.BatchNorm2d(channels),\n",
    "        neuron.IFNode(surrogate_function=surrogate.ATan()),\n",
    "        layer.AvgPool2d(2, 2),  # 7 * 7\n",
    "\n",
    "        layer.Flatten(),\n",
    "        layer.Linear(channels * 7 * 7, channels * 4 * 4, bias=False),\n",
    "        neuron.IFNode(surrogate_function=surrogate.ATan()),\n",
    "\n",
    "        layer.Linear(channels * 4 * 4, 10, bias=False),\n",
    "        neuron.IFNode(surrogate_function=surrogate.ATan()),\n",
    "        )\n",
    "        \n",
    "        functional.set_step_mode(self, step_mode='m')\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x.shape = [N, C, H, W]\n",
    "        # self.output_shape(x)\n",
    "        x_seq = (x.unsqueeze(0)).repeat(self.T, 1, 1, 1, 1)  # [N, C, H, W] -> [T, N, C, H, W]\n",
    "        x_seq = self.conv_fc(x_seq)\n",
    "        fr = x_seq.mean(0)\n",
    "        return fr\n",
    "    \n",
    "    def spiking_encoder(self):\n",
    "        return self.conv_fc[0:3]\n",
    "    \n",
    "    def output_shape(self,x):\n",
    "        for layer in self.conv_fc:\n",
    "            x = layer(x)\n",
    "            print(x.shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bada749-d3b6-4465-8fb7-8ee70d8b3448",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CSNN(T = T, channels = channels, use_cupy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dbd7df7-d976-4ac4-98d2-528e1a83a119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSNN(\n",
      "  (conv_fc): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, step_mode=m)\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, step_mode=m)\n",
      "    (2): IFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=m, backend=torch\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0, step_mode=m)\n",
      "    (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, step_mode=m)\n",
      "    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, step_mode=m)\n",
      "    (6): IFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=m, backend=torch\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "    (7): AvgPool2d(kernel_size=2, stride=2, padding=0, step_mode=m)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1, step_mode=m)\n",
      "    (9): Linear(in_features=392, out_features=128, bias=False)\n",
      "    (10): IFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=m, backend=torch\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "    (11): Linear(in_features=128, out_features=10, bias=False)\n",
      "    (12): IFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=m, backend=torch\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7768f09-b93b-4720-8ede-d9ab61453ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e3eeaf-fde7-4591-add1-6d1b9359d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ab5a01-ae5b-4c11-8996-edce6cd2eef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a9c9423-802e-47ed-a27d-08e53f598f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e5c83-f0a1-4b9e-b042-82527c8da478",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b15df962-6897-4c9c-8852-38c82d6a95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(start_epoch, epochs):\n",
    "#     start_time = time.time()\n",
    "#     net.train()\n",
    "#     train_loss = 0\n",
    "#     train_acc = 0\n",
    "#     train_samples = 0\n",
    "#     for img, label in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         img = img.to(device)\n",
    "#         label = label.to(device)\n",
    "#         label_onehot = F.one_hot(label, 10).float()\n",
    "#         out_fr = net(img)\n",
    "#         loss = F.mse_loss(out_fr, label_onehot)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         train_samples += label.numel()\n",
    "#         train_loss += loss.item() * label.numel()\n",
    "#         train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "        \n",
    "#         functional.reset_net(net)\n",
    "        \n",
    "#     train_time = time.time()\n",
    "#     train_speed = train_samples / (train_time - start_time)\n",
    "#     train_loss /= train_samples\n",
    "#     train_acc /= train_samples\n",
    "\n",
    "#     writer.add_scalar('train_loss', train_loss, epoch)\n",
    "#     writer.add_scalar('train_acc', train_acc, epoch)\n",
    "#     lr_scheduler.step()\n",
    "    \n",
    "#     net.eval()\n",
    "#     test_loss = 0\n",
    "#     test_acc = 0\n",
    "#     test_samples = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for img, label in test_loader:\n",
    "#             img = img.to(device)\n",
    "#             label = label.to(device)\n",
    "#             label_onehot = F.one_hot(label, 10).float()\n",
    "#             out_fr = net(img)\n",
    "#             loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "#             test_samples += label.numel()\n",
    "#             test_loss += loss.item() * label.numel()\n",
    "#             test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "#             functional.reset_net(net)\n",
    "#         test_time = time.time()\n",
    "#         test_speed = test_samples / (test_time - train_time)\n",
    "#         test_loss /= test_samples\n",
    "#         test_acc /= test_samples\n",
    "#         writer.add_scalar('test_loss', test_loss, epoch)\n",
    "#         writer.add_scalar('test_acc', test_acc, epoch)\n",
    "    \n",
    "#     save_max = False\n",
    "#     if test_acc > max_test_acc:\n",
    "#         max_test_acc = test_acc\n",
    "#         save_max = True\n",
    "            \n",
    "#     checkpoint = {\n",
    "#         'net': net.state_dict(),\n",
    "#         'optimizer': optimizer.state_dict(),\n",
    "#         'lr_scheduler': lr_scheduler.state_dict(),\n",
    "#         'epoch': epoch,\n",
    "#         'max_test_acc': max_test_acc\n",
    "#     }\n",
    "\n",
    "#     if save_max:\n",
    "#         torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_max.pth'))\n",
    "\n",
    "#     torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_latest.pth'))\n",
    "    \n",
    "#     print(f'epoch = {epoch}, train_loss ={train_loss: .4f}, train_acc ={train_acc: .4f}, test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}, max_test_acc ={max_test_acc: .4f}')\n",
    "#     print(f'train speed ={train_speed: .4f} images/s, test speed ={test_speed: .4f} images/s')\n",
    "#     print(f'escape time = {(datetime.datetime.now() + datetime.timedelta(seconds=(time.time() - start_time) * (epochs - epoch))).strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f70e10-8472-4e98-be35-08ebbb60bb1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save Checkpoint for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ac150e-f59c-43b2-ae5d-e867f9f02662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_quan, fdir, num_layer):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_quan:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, f'model_spikingjelly_quan_{num_layer}.pth.tar'))\n",
    "    else:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, f'model_spikingjelly_{num_layer}.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d331d6b3-1923-443b-b58f-5aff3c5566ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36672e2a-3dff-4b1e-aa4c-7e9fe7de597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_checkpoint({'state_dict': net.state_dict(),}, 0, fdir, len(net.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd428d6-1777-4e6b-b2c3-3573c9853ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_parameters = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "# print(f\"number of params: {n_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a57e9c8-21fa-4866-999e-b3e6e6b3eec3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0c190b3-a638-4d34-99c7-fe75b42b66d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, test_loader, device):\n",
    "    start_time = time.time()\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    test_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in test_loader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            label_onehot = F.one_hot(label, 10).float()\n",
    "            out_fr = net(img)\n",
    "            print(out_fr.shape)\n",
    "            return \n",
    "            loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "            test_samples += label.numel()\n",
    "            test_loss += loss.item() * label.numel()\n",
    "            test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "            functional.reset_net(net)\n",
    "        test_time = time.time()\n",
    "        test_speed = test_samples / (test_time - start_time)\n",
    "        test_loss /= test_samples\n",
    "        test_acc /= test_samples\n",
    "        writer.add_scalar('test_loss', test_loss)\n",
    "        writer.add_scalar('test_acc', test_acc)\n",
    "    \n",
    "    print(f'test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}')\n",
    "    print(f'test speed ={test_speed: .4f} images/s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62496625-f1f9-4bea-bae9-039f8f626a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "validate(net, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e881ae2f-24ef-4a4e-8706-67437246f425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_parameters = sum(p.numel() for p in net.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0da9dd0e-5bd7-4743-97ed-e4f6804f5ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 3, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv_fc[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a5e27d2-4276-4a91-91df-8e16dbe565f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "8\n",
      "8\n",
      "576\n",
      "8\n",
      "8\n",
      "50176\n",
      "1280\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7656bad2-842d-4d2e-8e87-934ac4c06786",
   "metadata": {},
   "source": [
    "### Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41180649-5abf-4831-ade3-9f80a1f1358b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CSNN(\n",
       "  (conv_fc): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, step_mode=m)\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, step_mode=m)\n",
       "    (2): IFNode(\n",
       "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=m, backend=torch\n",
       "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
       "    )\n",
       "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0, step_mode=m)\n",
       "    (4): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False, step_mode=m)\n",
       "    (5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, step_mode=m)\n",
       "    (6): IFNode(\n",
       "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=m, backend=torch\n",
       "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
       "    )\n",
       "    (7): AvgPool2d(kernel_size=2, stride=2, padding=0, step_mode=m)\n",
       "    (8): Flatten(start_dim=1, end_dim=-1, step_mode=m)\n",
       "    (9): Linear(in_features=392, out_features=128, bias=False)\n",
       "    (10): IFNode(\n",
       "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=m, backend=torch\n",
       "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
       "    )\n",
       "    (11): Linear(in_features=128, out_features=10, bias=False)\n",
       "    (12): IFNode(\n",
       "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=m, backend=torch\n",
       "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path = '/home/keli/code/CRI_Mapping/result/model_spikingjelly_14_s.pth.tar'\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "net_1 = CSNN(T = T, channels = channels, use_cupy=False)\n",
    "net_1.load_state_dict(checkpoint['state_dict'])\n",
    "net_1.eval()\n",
    "net_1.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37aec428-71c5-468b-a53d-8fbaefde4ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "validate(net_1, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6e706-c838-43ee-be6d-0a6d8da0f971",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "647121c4-0926-4f09-a2af-a8dabd46c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = BN_Folder()  #Fold the BN layer \n",
    "net_bn = bn.fold(net_1.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa0eaa81-7f78-4b0b-b9a3-36a72f1676b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(net_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29f44a9e-8da0-4c03-9e42-92c56a478eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "validate(net_bn, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e188caf7-fcec-48a8-a580-512f60f8823a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized:  conv_fc\n",
      "Quantized:  0\n",
      "Quantized:  1\n",
      "Quantized:  2\n",
      "Quantized:  3\n",
      "Quantized:  4\n",
      "Quantized:  5\n",
      "Quantized:  6\n",
      "Quantized:  7\n",
      "Quantized:  8\n",
      "Quantized:  9\n",
      "Quantized:  10\n",
      "Quantized:  11\n",
      "Quantized:  12\n",
      "Quantization time: 0.0020759105682373047\n",
      "Quantization time: 0.0036220550537109375\n"
     ]
    }
   ],
   "source": [
    "quan_fun = Quantize_Network(dynamic_alpha = False) # weight_quantization\n",
    "net_quan = quan_fun.quantize(net_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3ae299d-dc25-4ce6-9690-7fd34edc34cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(net_quan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd618e88-8663-41da-ade4-e18e2d6790c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "validate(net_quan, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf687861-7a9d-4cee-a5b0-251e35ffa5e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in net:  1\n",
      "Number of layers in net:  13\n",
      "Constructing Axons from Conv2d Layer\n",
      "Input layer shape(infeature, outfeature): [ 1 28 28] [ 8 28 28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 8 bias axons for input layer.\n",
      "Numer of neurons: 0, number of axons: 792\n",
      "Converting Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), step_mode=m) takes 0.0696554183959961\n",
      "Constructing hidden avgpool layer\n",
      "Hidden layer shape(infeature, outfeature): (8, 28, 28) [ 8 14 14]\n",
      "Neuron_offset: 6272\n",
      "Last output: 7839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 708.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of neurons: 6272, number of axons: 792\n",
      "Constructing Neurons from Conv2d Layer\n",
      "Hidden layer shape(infeature, outfeature): (8, 14, 14) [ 8 14 14]\n",
      "Neuron_offset: 7840\n",
      "Last output: ['9394' '9395' '9396' '9397' '9398' '9399' '9400' '9401' '9402' '9403'\n",
      " '9404' '9405' '9406' '9407']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 77.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 8 bias axons for input layer.\n",
      "Numer of neurons: 7840, number of axons: 800\n",
      "Converting Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), step_mode=m) takes 0.10711503028869629\n",
      "Constructing hidden avgpool layer\n",
      "Hidden layer shape(infeature, outfeature): (8, 14, 14) [8 7 7]\n",
      "Neuron_offset: 9408\n",
      "Last output: 9799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 2834.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of neurons: 9408, number of axons: 800\n",
      "Constructing Neurons from Linear Layer\n",
      "Hidden layer shape(infeature, outfeature):  392 128\n",
      "Neuron_offset: 9800\n",
      "Last output: 9927\n",
      "curr_neuron_offset, next_neuron_offset: (9408, 9800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of neurons: 9800, number of axons: 800\n",
      "Constructing Neurons from Linear Layer\n",
      "Hidden layer shape(infeature, outfeature):  128 10\n",
      "Neuron_offset: 9928\n",
      "Last output: 9937\n",
      "curr_neuron_offset, next_neuron_offset: (9800, 9928)\n",
      "Instantiate output neurons\n",
      "Numer of neurons: 9938, number of axons: 800\n"
     ]
    }
   ],
   "source": [
    "cri_convert = CRI_Converter(4, 0, 11, np.array((1, 28, 28)),'spikingjelly') # num_steps, input_layer, output_layer, input_size\n",
    "cri_convert.layer_converter(net_quan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d7dccaa-67a6-43e1-afcc-e56a37adc5ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9928',\n",
       " '9929',\n",
       " '9930',\n",
       " '9931',\n",
       " '9932',\n",
       " '9933',\n",
       " '9934',\n",
       " '9935',\n",
       " '9936',\n",
       " '9937']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cri_convert.output_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de1da326-9411-4e00-8a8c-de062f24db07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cri_convert.axon_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37a5220d-792d-44e7-9248-03c073a83685",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9938"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cri_convert.neuron_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c56e6b24-0724-436f-abf2-18484f3f19e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(cri_convert.neuron_dict['6272']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846478b-fadf-4829-86d7-99ae165b8294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
